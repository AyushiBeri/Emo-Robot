# Emo-Robot
I built an emotionally intelligent social feeding robot called Emo-Robot, designed to assist individuals who face challenges in feeding themselves—such as the elderly, differently-abled, or emotionally vulnerable people. This robot not only helps with feeding but also acts as a companion by understanding and responding to the user's emotional state. It includes features like a multilingual empathetic voice assistant (supports Hindi and Hindlish), facial emotion detection, and voice-based emotion analysis to recognize feelings such as happiness, sadness, fear, and anger in real time. A major highlight is the visual and audio emotion fusion module, which tracks emotional patterns over time to detect early signs of depression and shares this data with caregivers or doctors (if authorized). The feeding system is smart—it uses computer vision to detect mouth position, eye gaze, and obstacles, and only feeds when it’s safe. It can also detect food types, estimate calories and volume per spoon, and maintain a real-time diet log. I used tools and frameworks like MediaPipe for face and gaze tracking, OpenCV for computer vision, Python for integration, text-to-speech and speech recognition APIs for voice interaction, and deployed all modules on a Raspberry Pi 5 to make the system compact and efficient. Emo-Robot combines physical assistance with emotional care, offering a more human-like interaction and holistic support system
